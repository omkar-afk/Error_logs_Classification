# -*- coding: utf-8 -*-
"""Another copy of Mindspark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gs-6TXRFWnklRqqbxB2d4KCUJZzq9V3D
"""

# Importing required libraries
import os
import re
from datetime import datetime
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import BertTokenizer, BertModel
import torch
import numpy as np  # Import numpy and define the alias 'np'

import re
from collections import Counter

# Load the dataset
file_path = '/content/logss.txt'  # Replace with the path to your txt file
with open(file_path, 'r') as file:
    log_data = file.readlines()

# Regular expression to match error levels
error_level_pattern = re.compile(r'\[(crit|error|warn|info|alert|emerg|notice|debug)\]')

# Extract error levels
error_levels = [error_level_pattern.search(line).group(1) for line in log_data if error_level_pattern.search(line)]

# Count occurrences of each error level
error_level_counts = Counter(error_levels)

# Display the counts
print("Error Level Distribution:")
for level, count in error_level_counts.items():
    print(f"{level}: {count}")

# Define the path to the log file
log_file_path = "/content/logss.txt"

# Initialize an empty list to store the log entries
log_entries = []

# Open and read the log file
with open(log_file_path, "r") as file:
    # Read each line from the file
    for line in file:
        # Strip any leading/trailing whitespace characters from the log entry
        log_entry = line.strip()
        # Append the cleaned log entry to the list
        log_entries.append(log_entry)

# Check the number of log entries loaded
print(f"Total number of log entries loaded: {len(log_entries)}")

# Optionally, print the first few log entries to verify the data is loaded correctly
for i, entry in enumerate(log_entries[:5]):  # Displaying only the first 5 entries
    print(f"Log Entry {i+1}: {entry}")

# Define a regex pattern to extract the required components from each log entry
log_pattern = re.compile(r"\[(?P<timestamp>.*?)\] \[(?P<level>\w+)\] \[client (?P<client_ip>\d+\.\d+\.\d+\.\d+)\] (?P<message>.+)")

# Initialize a list to store parsed log details
parsed_logs = []

# Iterate through each log entry and parse it
for log in log_entries:
    match = log_pattern.match(log)
    if match:
        # Extract components using named groups from the regex
        log_details = {
            "level": match.group("level"),
            "message": match.group("message")
        }

        # Append the parsed log details to the list
        parsed_logs.append(log_details)

# Check the number of successfully parsed logs
print(f"Total parsed logs: {len(parsed_logs)}")

# Optionally, print the first few parsed logs to verify
for i, log in enumerate(parsed_logs[:5]):
    print(f"Parsed Log {i+1}: {log}")

# Define severity and impact mappings based on log levels
severity_mapping = {
    "emerg": "High",
    "alert": "High",
    "crit": "High",
    "error": "Medium",
    "warn": "Low",
    "info": "Low",
}

impact_mapping = {
    "emerg": "System Unusable",
    "alert": "Immediate Action Required",
    "crit": "Critical Conditions",
    "error": "Error Condition",
    "warn": "Warning Condition",
    "info": "Informational Condition",
}

# Augment each parsed log with severity and impact ratings
for log in parsed_logs:
    log_level = log["level"]

    # Assign severity and impact based on the log level
    log["severity"] = severity_mapping.get(log_level, "Unknown")
    log["impact"] = impact_mapping.get(log_level, "Unknown")

# Check the first few logs to verify severity and impact have been added
for i, log in enumerate(parsed_logs[:5]):
    print(f"Log {i+1} with Severity and Impact: {log}")

# Create a DataFrame from parsed logs for easier manipulation
log_df = pd.DataFrame(parsed_logs)

# Display the first few rows of the DataFrame to verify
print("Initial DataFrame with Parsed Logs:")
print(log_df.head())

# Extract relevant features
features_df = log_df[['level', 'message', 'severity', 'impact']]

# Display the features DataFrame to verify
print("Features DataFrame:")
print(features_df.head())

# Initialize LabelEncoders for the target variables
from sklearn.preprocessing import LabelEncoder
level_encoder = LabelEncoder()
severity_encoder = LabelEncoder()
impact_encoder = LabelEncoder()

# Fit and transform the target variables
features_df['level'] = level_encoder.fit_transform(features_df['level'])
features_df['severity'] = severity_encoder.fit_transform(features_df['severity'])
features_df['impact'] = impact_encoder.fit_transform(features_df['impact'])

# Display the encoded DataFrame to verify
print("Encoded Features DataFrame:")
print(features_df.head())

# Initialize the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=500)  # Adjust max_features as needed

# Fit and transform the error messages to create TF-IDF features
tfidf_features = tfidf_vectorizer.fit_transform(features_df['message']).toarray()

# Create a DataFrame for TF-IDF features
tfidf_features_df = pd.DataFrame(tfidf_features, columns=tfidf_vectorizer.get_feature_names_out())

# Combine TF-IDF features with the existing features DataFrame
final_features_df = pd.concat([features_df.reset_index(drop=True), tfidf_features_df.reset_index(drop=True)], axis=1)

# Display the final features DataFrame to verify
print("Final Features DataFrame with TF-IDF Features:")
print(final_features_df.head())

# Initialize the BERT Tokenizer and Model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Function to generate BERT embeddings
def get_bert_embeddings(messages):
    embeddings = []
    for message in messages:
        # Tokenize and encode the message
        inputs = tokenizer(message, return_tensors='pt', truncation=True, padding=True, max_length=512)

        # Get BERT model outputs
        with torch.no_grad():
            outputs = model(**inputs)

        # Mean pooling of the last hidden state to get sentence embeddings
        sentence_embedding = outputs.last_hidden_state.mean(dim=1).numpy()
        embeddings.append(sentence_embedding)

    return np.vstack(embeddings)  # Stack all embeddings into a single array

# Generate BERT embeddings for the error messages
bert_embeddings = get_bert_embeddings(features_df['message'].tolist())

# Create a DataFrame for BERT embeddings
bert_df = pd.DataFrame(bert_embeddings)

# Combine BERT embeddings with the existing features DataFrame
final_features_df = pd.concat([features_df.reset_index(drop=True), bert_df.reset_index(drop=True)], axis=1)

# Display the final features DataFrame to verify
print("Final Features DataFrame with BERT Embeddings:")
print(final_features_df.head())

from sklearn.model_selection import train_test_split

# Define the target variables
target_columns = ['level', 'severity', 'impact']

# Separate features and targets
X = final_features_df.drop(columns=target_columns)  # Drop target columns
y = final_features_df[target_columns]

# Split the dataset into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import torch
from transformers import BertTokenizer

# Create a Dataset class for BERT
class LogDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  # Ensure labels are float for BCE
        return item

    def __len__(self):
        return len(self.labels)

# Prepare the BERT inputs
X_bert = features_df['message'].tolist()  # Using the original messages for BERT

# Define the target columns
target_columns = ['level', 'severity', 'impact']

# Initialize the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Train and evaluate BERT for each target variable
for target in target_columns:
    print(f"Training BERT on target variable: {target}")

    # Prepare target variable for the current iteration
    y_bert = features_df[target].tolist()  # Choose the current target variable to classify

    # Split the dataset into training and testing sets (80%-20%)
    X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(X_bert, y_bert, test_size=0.2, random_state=42)

    # Tokenize the input messages
    train_encodings = tokenizer(X_train_bert, truncation=True, padding=True, max_length=512)
    test_encodings = tokenizer(X_test_bert, truncation=True, padding=True, max_length=512)

    # Create Dataset objects
    train_dataset = LogDataset(train_encodings, y_train_bert)
    test_dataset = LogDataset(test_encodings, y_test_bert)

    # Set training arguments for fine-tuning BERT
    training_args = TrainingArguments(
        output_dir='./results',               # Output directory
        num_train_epochs=3,                   # Total number of training epochs
        per_device_train_batch_size=16,       # Batch size per device during training
        per_device_eval_batch_size=64,        # Batch size for evaluation
        warmup_steps=500,                      # Number of warmup steps for learning rate scheduler
        weight_decay=0.01,                    # Strength of weight decay
        logging_dir='./logs',                  # Directory for storing logs
        logging_steps=10,
        eval_strategy="epoch"                  # Updated evaluation strategy
    )

    # Initialize the BERT model for sequence classification
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # Set num_labels=1 for binary/multi-label

    # Create Trainer object
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset
    )

    # Train the model
    trainer.train()

    # Evaluate the model
    eval_results = trainer.evaluate()
    print(f"Evaluation Results for {target}:")
    print(eval_results)

    # Run predictions for the first few instances
    predictions = trainer.predict(test_dataset)
    # Getting the first few predictions (e.g., first 5)
    first_few_predictions = predictions.predictions[:5]
    print(f"First few predictions for {target}:", first_few_predictions)

from sklearn.metrics import accuracy_score, f1_score, classification_report

# Initialize a dictionary to hold metrics for each target variable
metrics = {}

# Iterate over each target variable to compute metrics
for target in target_columns:
    print(f"Calculating metrics for target variable: {target}")

    # Prepare target variable for evaluation
    y_bert = features_df[target].tolist()  # Get the current target variable to classify

    # Split the dataset into training and testing sets (80%-20%)
    X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(X_bert, y_bert, test_size=0.2, random_state=42)

    # Tokenize the input messages for the test set
    test_encodings = tokenizer(X_test_bert, truncation=True, padding=True, max_length=512)
    test_dataset = LogDataset(test_encodings, y_test_bert)

    # Run predictions for the test dataset
    predictions = trainer.predict(test_dataset)
    pred_labels = predictions.predictions.argmax(-1)  # Get predicted labels

    # Calculate metrics
    accuracy = accuracy_score(y_test_bert, pred_labels)
    f1 = f1_score(y_test_bert, pred_labels, average='weighted')
    report = classification_report(y_test_bert, pred_labels, output_dict=True)

    # Store metrics in the dictionary
    metrics[target] = {
        'accuracy': accuracy,
        'f1_score': f1,
        'classification_report': report
    }

    # Print metrics
    print(f"Accuracy for {target}: {accuracy:.2f}")
    print(f"F1 Score for {target}: {f1:.2f}")
    print("Classification Report:")
    print(classification_report(y_test_bert, pred_labels))

# Optionally, you can access the metrics dictionary for further analysis

model.save_pretrained("path_to_save_model")
tokenizer.save_pretrained("path_to_save_model")

# Function to classify a random error message into level, severity, and impact
def classify_error(error_message):
    # Move model to the appropriate device
    model.to('cuda' if torch.cuda.is_available() else 'cpu')

    # Tokenize and encode the input message
    inputs = tokenizer(error_message, return_tensors='pt', truncation=True, padding=True, max_length=512)

    # Move inputs to the appropriate device
    inputs = {key: val.to('cuda' if torch.cuda.is_available() else 'cpu') for key, val in inputs.items()}

    # Get predictions from the model
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # Convert logits to probabilities and get the predicted class
    predicted_class = logits.argmax(-1).item()

    # Decode predicted level, severity, and impact
    predicted_level = level_encoder.inverse_transform([predicted_class])[0]
    predicted_severity = severity_mapping.get(predicted_level, "Unknown")
    predicted_impact = impact_mapping.get(predicted_level, "Unknown")

    return predicted_level, predicted_severity, predicted_impact

# Example usage of the classify_error function
random_input = "[Thu Jun 03 16:31:45 2010] [emerg] [client 203.0.113.50] System reached max number of allowed processes"  # Replace with your error message
predicted_level, predicted_severity, predicted_impact = classify_error(random_input)

print(f"Predicted Level: {predicted_level}")
print(f"Predicted Severity: {predicted_severity}")
print(f"Predicted Impact: {predicted_impact}")